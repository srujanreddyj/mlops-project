{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vCJ3pvJKY_xV"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "### Problem\n",
        "\n",
        "In the project this week, we will build a machine learning text classifier to predict news categories from the news article text. \n",
        "\n",
        "1. We will iterate on classification models with increasing level of complexity and improved performance: N-gram models, pre-trained Transformer models, and third-party hosted Large Language Models (LLMs).\n",
        "\n",
        "2. We will look at the impact of labeled dataset size and composition on model performance. The labeled dataset will be used for training in case of N-gram models and pre-trained Transformers, and for selecting examples for in-context few-shot learning for LLMs.\n",
        "\n",
        "3. [advanced] As an extension, we will explore how to augment data efficiently to your existing training data (efficiency measured as improvement in performance normalized by volume of data augmented). \n",
        "\n",
        "Throughout the project there are suggested model architectures that we expect to work reasonably well for this problem. But if you wish to extend/modify any part of this pipeline, or explore new model architectures you should definitely feel free to do so.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FNP8FSfZIed"
      },
      "source": [
        "## Step1: Prereqs & Installation\n",
        "\n",
        "Download & Import all the necessary libraries we need throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l1LsWxD0ZF3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (1.24.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: sentence-transformers in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: numpy in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (1.24.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: requests in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.30.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\n",
            "Requirement already satisfied: filelock in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: jinja2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: joblib in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: matplotlib in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (3.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (4.39.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from matplotlib) (1.24.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: langchain in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (0.0.192)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (1.24.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (1.10.8)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.30.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (2.0.15)\n",
            "Requirement already satisfied: langchainplus-sdk<0.0.5,>=0.0.4 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from langchain) (0.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: openai in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai) (2.30.0)\n",
            "Requirement already satisfied: aiohttp in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests>=2.20->openai) (3.1.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Requirement already satisfied: wandb in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (0.15.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (2.30.0)\n",
            "Requirement already satisfied: PyYAML in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setproctitle in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (4.23.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from wandb) (1.25.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 23.1.2 is available.\n",
            "You should consider upgrading via the '/Users/sjabbireddy/.pyenv/versions/3.10.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install all the required dependencies for the project\n",
        "\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install sentence-transformers\n",
        "!pip install matplotlib\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yiDpaCRTZOKL"
      },
      "outputs": [],
      "source": [
        "# Package imports that will be needed for this project\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from pprint import pprint\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# [TO BE IMPLEMENTED] \n",
        "# Add any other imports needed below depending on the model architectures you are using. For e.g.\n",
        "# from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "p9asDVPMZlf3"
      },
      "outputs": [],
      "source": [
        "with open('apikey.json') as f:\n",
        "   apikey = json.load(f)\n",
        "\n",
        "OPENAI_API_KEY = apikey['API_KEY_USER']\n",
        "\n",
        "# Global Constants\n",
        "LABEL_SET = [\n",
        "    'Business',\n",
        "    'Sci/Tech',\n",
        "    'Software and Developement',\n",
        "    'Entertainment',\n",
        "    'Sports',\n",
        "    'Health',\n",
        "    'Toons',\n",
        "    'Music Feeds'\n",
        "]\n",
        "\n",
        "WORD_VECTOR_MODEL = 'glove-wiki-gigaword-100'\n",
        "SENTENCE_TRANSFORMER_MODEL = 'all-mpnet-base-v2'\n",
        "\n",
        "TRAIN_SIZE_EVALS = [500, 1000, 10000, 25000]\n",
        "EPS = 0.001\n",
        "SEED = 0\n",
        "\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gLu2IBiqZsgs"
      },
      "source": [
        "## Step 2: Download & Load Datasets \n",
        "\n",
        "[AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) is a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. The news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626). The dataset has the text description (summary) of the news article along with some metadata. **For this project, we will use a slightly modified (cleaned up) version of this dataset** \n",
        "\n",
        "Schema:\n",
        "* Source - News publication source\n",
        "* URL - URL of the news article\n",
        "* Title - Title of the news article\n",
        "* Description - Summary description of the news article\n",
        "* Category (Label) - News category\n",
        "\n",
        "Sample row in this dataset:\n",
        "```\n",
        "{\n",
        "    'description': 'A capsule carrying solar material from the Genesis space '\n",
        "                'probe has made a crash landing at a US Air Force training '\n",
        "                'facility in the US state of Utah.',\n",
        "    'id': 86273,\n",
        "    'label': 'Entertainment',\n",
        "    'source': 'Voice of America',\n",
        "    'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
        "    'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'\n",
        " }\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jFGaBYdSZtqM"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "DIRECTORY_NAME = \"data\"\n",
        "DOWNLOAD_URL = 'https://corise-mlops.s3.us-west-2.amazonaws.com/project1/agnews.zip'\n",
        "\n",
        "def download_dataset():\n",
        "    \"\"\"\n",
        "    Download the dataset. The zip contains three files: train.json, test.json and unlabeled.json \n",
        "    \"\"\"\n",
        "    http_response = urlopen(DOWNLOAD_URL)\n",
        "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
        "    zipfile.extractall(path=DIRECTORY_NAME)\n",
        "\n",
        "# Expensive operation so we should just do this once\n",
        "download_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zYnT4BIcZ5vX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Dataset train with 25000 rows\n",
            "Loaded Dataset test with 5000 rows\n",
            "Loaded Dataset augment with 150000 rows\n",
            "\n",
            "Example train row:\n",
            "\n",
            "{'description': 'A capsule carrying solar material from the Genesis space '\n",
            "                'probe has made a crash landing at a US Air Force training '\n",
            "                'facility in the US state of Utah.',\n",
            " 'id': 86273,\n",
            " 'label': 'Entertainment',\n",
            " 'source': 'Voice of America',\n",
            " 'title': 'Capsule from Genesis Space Probe Crashes in Utah Desert',\n",
            " 'url': 'http://www.sciencedaily.com/releases/2004/09/040908090621.htm'}\n",
            "\n",
            "Example test row:\n",
            "\n",
            "{'description': 'European Union regulators will decide Tuesday whether Oracle '\n",
            "                \"Corp.'s hostile \\\\$7.7 billion bid for rival business \"\n",
            "                \"software concern PeopleSoft Inc. can proceed, the EU's \"\n",
            "                'antitrust chief said Friday.',\n",
            " 'id': 278781,\n",
            " 'label': 'Sci/Tech',\n",
            " 'source': 'Washington Post Tech',\n",
            " 'title': \"EU to Rule Tuesday on Oracle's Bid for PeopleSoft\",\n",
            " 'url': 'http://www.washingtonpost.com/wp-dyn/articles/A53747-2004Oct22.html?nav=rss_technology'}\n"
          ]
        }
      ],
      "source": [
        "Datasets = {}\n",
        "\n",
        "for ds in ['train', 'test', 'augment']:\n",
        "    with open('data/{}.json'.format(ds), 'r') as f:\n",
        "        Datasets[ds] = json.load(f)\n",
        "    print(\"Loaded Dataset {0} with {1} rows\".format(ds, len(Datasets[ds])))\n",
        "\n",
        "print(\"\\nExample train row:\\n\")\n",
        "pprint(Datasets['train'][0])\n",
        "\n",
        "print(\"\\nExample test row:\\n\")\n",
        "pprint(Datasets['test'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TcwebhuYZ8Kb"
      },
      "outputs": [],
      "source": [
        "X_train, Y_train = [], []\n",
        "X_test, Y_true = [], []\n",
        "X_augment, Y_augment = [], []\n",
        "\n",
        "for row in Datasets['train']:\n",
        "    X_train.append(row['description'])\n",
        "    Y_train.append(row['label'])\n",
        "\n",
        "for row in Datasets['test']:\n",
        "    X_test.append(row['description'])\n",
        "    Y_true.append(row['label'])\n",
        "\n",
        "for row in Datasets['augment']:\n",
        "    X_augment.append(row['description'])\n",
        "    Y_augment.append(row['label'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KoayaMr8aBwp"
      },
      "source": [
        "## Step 3: [Modeling part 1] N-gram model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "n2zIahH6aHJ9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for training data size = 500\n",
            "Accuracy on test set: 0.3434\n",
            "Evaluating for training data size = 1000\n",
            "Accuracy on test set: 0.491\n",
            "Evaluating for training data size = 10000\n",
            "Accuracy on test set: 0.6474\n",
            "Evaluating for training data size = 25000\n",
            "Accuracy on test set: 0.7016\n"
          ]
        }
      ],
      "source": [
        "models = {}\n",
        "\n",
        "for n in TRAIN_SIZE_EVALS:\n",
        "    print(\"Evaluating for training data size = {}\".format(n))\n",
        "    X_train_i = X_train[:n]\n",
        "    Y_train_i = Y_train[:n]\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "    You have to replace it with a pipeline object which contains at least two steps:\n",
        "    (1) mapping the input document to an N-gram feature extractor. You can use feature extractors\n",
        "        provided by sklearn out of the box (e.g. CountVectorizer, TfidfTransformer)\n",
        "    (2) a classifier that predicts the class label using the feature output of first step\n",
        "\n",
        "    You can add other steps to preproces, post-process your data as you see fit. \n",
        "    You can also try any sklearn model architecture you want, but a linear classifier\n",
        "    will do just fine to start with\n",
        "\n",
        "    e.g. \n",
        "    pipeline = Pipeline([\n",
        "        ('featurizer', <your WordVectorFeaturizer class instance here>),\n",
        "        ('classifier', <your sklearn classifier class instance here>)\n",
        "    ])\n",
        "\n",
        "    Reference: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000, C=0.1))\n",
        "    ])\n",
        "    \n",
        "    # train\n",
        "    pipeline.fit(X_train_i, Y_train_i)\n",
        "    # predict\n",
        "    Y_pred_i = pipeline.predict(X_test)\n",
        "    # record results\n",
        "    models[n] = {\n",
        "        'pipeline': pipeline,\n",
        "        'test_predictions': Y_pred_i,\n",
        "        'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "        'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "        'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "    }\n",
        "    print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Side-Note: \n",
        "Recieved following error for training data size = 10000 and above \n",
        "/Users/sjabbireddy/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "\n",
        "The error message says that the LogisticRegression object failed to converge after a certain number of iterations. This can happen for a number of reasons, including:\n",
        "* The data is not linearly separable.\n",
        "* The data is too noisy.\n",
        "* The regularization parameter is too high.\n",
        "\n",
        "To fix this error, following are the options:\n",
        "\n",
        "* Increase the number of iterations.\n",
        "* Use a different regularization parameter.\n",
        "* Use a different algorithm, such as a decision tree or a random forest.\n",
        "\n",
        "Here are some specific changes you can make:\n",
        "\n",
        "* Change the max_iter parameter of the LogisticRegression object to a higher value.\n",
        "* Change the C parameter of the LogisticRegression object to a lower value.\n",
        "* Use a different algorithm, such as a decision tree or a random forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OBxVNbWBaMhc"
      },
      "source": [
        "## Step 4: [Modeling part 2] Pretrained Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "27TJGTZfavys"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "example_encoding:\n",
            " [ 2.25026626e-02 -7.82916993e-02 -2.30307151e-02 -5.10002859e-03\n",
            " -8.03404301e-02  3.91322300e-02  1.13428347e-02  3.46491998e-03\n",
            " -2.94575840e-02 -1.88930500e-02  9.47433040e-02  2.92748380e-02\n",
            "  3.94859649e-02 -4.63165380e-02  2.54245698e-02 -3.21999528e-02\n",
            "  6.21928051e-02  1.55591769e-02 -4.67794649e-02  5.03901392e-02\n",
            "  1.46114370e-02  2.31413841e-02  1.22067872e-02  2.50696782e-02\n",
            "  2.93659070e-03 -4.19822037e-02 -4.01036115e-03 -2.27843486e-02\n",
            " -7.68594025e-03 -3.31090614e-02  3.22118476e-02 -2.09993217e-02\n",
            "  1.16730919e-02 -9.85073894e-02  1.77932668e-06 -2.29932163e-02\n",
            " -1.31141404e-02 -2.80222669e-02 -6.99970424e-02  2.59314720e-02\n",
            " -2.89502330e-02  8.76336992e-02 -1.20919039e-02  3.98605354e-02\n",
            " -3.31381746e-02  3.59109193e-02  3.46098952e-02  6.49784356e-02\n",
            " -3.00816000e-02  6.98187649e-02 -3.99512099e-03 -1.01595640e-03\n",
            " -3.50185521e-02 -4.36566696e-02  5.08026257e-02  4.68758643e-02\n",
            "  5.39663546e-02 -4.03008834e-02  3.20150354e-03  1.36618102e-02\n",
            "  3.82188521e-02 -3.23842815e-03 -7.84593867e-04 -1.71188228e-02\n",
            "  6.90434035e-03 -1.09236510e-02  8.63302127e-03 -1.82358548e-02\n",
            "  1.87930446e-02  1.54989650e-02  1.02149760e-02 -2.48376420e-03\n",
            "  1.03154108e-02  6.24887347e-02  3.60323675e-03 -6.26625679e-03\n",
            " -2.03405544e-02 -6.72349427e-03 -3.54771838e-02  3.43538299e-02\n",
            "  6.72282577e-02  9.06874314e-02  1.32440822e-02  2.06591729e-02\n",
            " -2.78684944e-02  4.29694913e-02 -4.66859564e-02  1.50116021e-02\n",
            " -6.62284270e-02 -2.27593035e-02 -6.24990799e-02 -2.58456096e-02\n",
            "  7.31334207e-04  1.14652691e-02  5.66384159e-02  2.06239964e-03\n",
            " -4.09248732e-02 -4.55051139e-02  1.66958403e-02 -8.31558034e-02\n",
            "  2.09065247e-03 -8.70915689e-03  1.07648601e-04  3.37445773e-02\n",
            "  5.60349459e-03 -1.66981164e-02  4.47909087e-02  6.31809840e-03\n",
            " -6.45904168e-02  5.29104061e-02  1.93018839e-02 -6.20148098e-03\n",
            " -1.18759997e-01  3.55963930e-02 -2.28864048e-02 -1.51872160e-02\n",
            " -5.92650101e-03 -1.57209288e-04  1.07070105e-02  3.86090158e-03\n",
            " -6.87014386e-02 -1.69752296e-02 -2.79729366e-02  2.80481018e-02\n",
            "  2.47792769e-02  1.20278997e-02 -6.86393455e-02  4.92764935e-02\n",
            "  1.87576115e-02 -2.42343601e-02 -2.05291212e-02 -1.07934242e-02\n",
            "  2.46493295e-02 -3.33323851e-02 -3.28397080e-02  2.91977972e-02\n",
            "  4.92032878e-02 -7.13362731e-03 -1.63390115e-02  1.78585411e-03\n",
            "  2.18068231e-02 -8.90232027e-02 -3.37051339e-02  5.77217760e-03\n",
            " -4.56565730e-02  3.39891352e-02  3.52784395e-02 -3.12627479e-02\n",
            "  8.10832158e-03  2.68614814e-02 -2.23908620e-03  2.81266663e-02\n",
            " -1.75384171e-02 -1.44590093e-02 -3.33480537e-02 -1.62954573e-02\n",
            "  9.70038325e-02 -8.11065920e-03 -2.46668048e-02 -5.87456822e-02\n",
            "  8.74837104e-04  1.67235658e-02  9.15385503e-03 -1.17981317e-03\n",
            " -2.93016410e-03  4.22471715e-03 -2.16529462e-02  4.29306440e-02\n",
            " -5.86095192e-02  3.13417315e-02 -1.29511987e-03 -1.11298952e-02\n",
            " -2.82019619e-02  8.77323598e-02  2.06880402e-02  1.41397696e-02\n",
            "  1.38229160e-02 -1.94184035e-02 -9.01035368e-02 -3.81485722e-03\n",
            " -2.91149225e-03  3.09754703e-02 -1.18768774e-02  1.88289359e-02\n",
            " -4.59065661e-02  4.98210490e-02 -8.39174911e-03 -4.29712906e-02\n",
            " -3.23598608e-02 -3.83801349e-02 -2.99748387e-02  3.69881429e-02\n",
            " -4.44588345e-03 -1.94783900e-02 -2.71527655e-02  2.43246518e-02\n",
            "  9.16473218e-04  5.85005693e-02  1.92715190e-02 -2.57291235e-02\n",
            "  4.08677757e-02  4.36857902e-03  5.13519980e-02  1.57080777e-02\n",
            " -2.46328916e-02 -9.79603361e-03  2.06121104e-03 -4.66644280e-02\n",
            "  3.19584310e-02 -3.73426490e-02  9.35151950e-02  1.85420867e-02\n",
            " -2.60214750e-02  8.05768650e-03 -6.38874917e-05 -4.74145310e-03\n",
            "  2.17361506e-02 -4.03623395e-02 -3.97235602e-02  6.60505146e-02\n",
            " -3.20185572e-02 -1.52356988e-02 -1.53095871e-02  5.58146741e-03\n",
            "  3.96784134e-02 -5.98881058e-02 -2.94910502e-02 -1.53480042e-02\n",
            " -3.32981274e-02 -1.35856252e-02 -2.23695338e-02  1.81124150e-03\n",
            " -2.53460894e-04  7.30920769e-03 -4.96328808e-02  3.74634154e-02\n",
            " -4.42486368e-02 -8.77882764e-02 -1.95525941e-02 -7.44620264e-02\n",
            " -5.28366724e-03 -8.59958772e-03  1.65658295e-02  1.99178904e-02\n",
            " -9.94193275e-03 -2.85206432e-03  7.21454620e-02 -1.99029315e-02\n",
            "  2.95140166e-02 -5.97201586e-02  5.00880815e-02 -2.54911594e-02\n",
            "  2.33916864e-02 -7.12681049e-03  7.38671189e-03 -7.17941523e-02\n",
            "  9.14909993e-04  2.19873302e-02  4.15905332e-03  1.79544427e-02\n",
            "  6.32213503e-02 -2.47936440e-03 -5.26579004e-03  2.34971251e-02\n",
            " -2.61955205e-02 -3.71229611e-02  2.15679146e-02 -5.85354641e-02\n",
            " -1.79578140e-02 -1.20004062e-02  8.96518526e-04 -1.47689795e-02\n",
            "  4.96945158e-02  6.97960425e-03  2.64366958e-02  4.61773314e-02\n",
            "  3.20434906e-02 -3.66006121e-02 -5.08430554e-03  6.88665956e-02\n",
            "  5.68004996e-02 -1.46778459e-02 -4.78474237e-02  1.21872108e-02\n",
            " -2.50421800e-02  3.12443189e-02 -1.79439168e-02 -3.05826962e-02\n",
            "  1.71712926e-03  7.02126920e-02  5.67383543e-02 -1.79368127e-02\n",
            "  2.43999995e-02 -2.86525842e-02 -1.15867388e-02 -2.70409230e-02\n",
            "  3.95130739e-02  4.29957211e-02  2.90971976e-02  2.80838683e-02\n",
            " -4.62749042e-02 -4.28294949e-03  1.19901784e-02 -1.20224822e-02\n",
            " -9.46935080e-03  2.35066116e-02 -3.00628021e-02 -1.69607513e-02\n",
            " -1.59739319e-03 -1.30610149e-02  5.35884164e-02  2.53782999e-02\n",
            "  2.60250028e-02  6.27413094e-02 -2.26462483e-02  6.58667786e-03\n",
            " -3.48779857e-02 -8.88991170e-03 -3.32266539e-02 -1.81600340e-02\n",
            " -6.45450642e-03  1.02021303e-02 -1.25164408e-02  4.20163311e-02\n",
            "  1.12151271e-02 -2.13346463e-02  1.05621312e-02  1.99820232e-02\n",
            "  1.83804315e-02  3.29696038e-03 -8.70436244e-03  1.90761685e-02\n",
            " -4.41013500e-02  9.57714841e-02  2.73615792e-02  1.76533572e-02\n",
            " -2.20417734e-02  3.70630957e-02 -6.52574701e-04 -1.44511480e-02\n",
            "  1.09790619e-02 -8.40484817e-03 -3.26197012e-03 -2.20720023e-02\n",
            " -1.90347992e-02 -1.60557982e-02 -4.08148803e-02  1.11608738e-02\n",
            " -6.02422729e-02 -6.96680993e-02 -1.73303355e-02  2.87934411e-02\n",
            " -6.79623261e-02 -3.13759595e-02 -5.51356412e-02 -2.03581676e-02\n",
            "  2.89012883e-02  1.37793850e-02  6.80497428e-03 -2.43215612e-03\n",
            "  7.21531734e-02 -1.17462093e-03 -3.57214734e-02  3.54788080e-02\n",
            " -1.96373113e-03 -7.76636414e-03  3.01939081e-02  1.85422301e-02\n",
            " -5.39994203e-02  3.32430154e-02  5.73024945e-03  1.33993067e-02\n",
            "  4.51611169e-03  4.88920659e-02 -3.14347185e-02  3.62169296e-02\n",
            "  3.65448520e-02 -4.79208902e-02 -1.44876279e-02  4.93126474e-02\n",
            "  2.86979564e-02 -5.51464111e-02  2.74743326e-02  1.27805443e-02\n",
            " -7.04632401e-02  7.69060478e-03 -5.24689024e-03 -5.33922948e-02\n",
            " -1.70808565e-02  4.77676392e-02  2.38064192e-02 -4.09797318e-02\n",
            " -1.27405897e-02  4.66343202e-02  5.03484346e-03  6.60547754e-03\n",
            "  2.90570222e-02  4.15973775e-02 -3.82126421e-02 -1.14388382e-02\n",
            "  1.71639808e-02  5.70879783e-03  1.07285567e-02 -1.80591922e-02\n",
            " -5.06380126e-02  4.54925559e-02  1.40738711e-02  4.25583310e-02\n",
            " -3.22351642e-02  4.17672582e-02  1.14987204e-02  3.92406061e-03\n",
            "  2.04459280e-02  1.52545515e-02  3.80403139e-02  2.54581049e-02\n",
            " -4.69274493e-03  1.83215067e-02  2.76015904e-02 -2.89156958e-02\n",
            " -4.98981141e-02 -1.61939599e-02  9.87022966e-02 -4.26362492e-02\n",
            " -1.88478157e-02 -1.07012065e-02 -3.21415104e-02  4.15322185e-02\n",
            " -2.38698982e-02  8.39930773e-03 -1.00908056e-03 -3.11339963e-02\n",
            " -3.86490636e-02 -3.06743085e-02 -3.88900824e-02 -3.65616679e-02\n",
            "  3.29435919e-03  2.00938862e-02  2.30733138e-02 -4.77465689e-02\n",
            "  8.55976809e-03  2.21940633e-02  1.49231151e-01 -1.91772394e-02\n",
            "  1.43476035e-02  4.39948738e-02 -2.27755611e-03  1.38112367e-03\n",
            "  3.23159546e-02  6.57534897e-02  2.26996914e-02  2.18100548e-02\n",
            " -3.00689284e-02  1.54186720e-02  6.95953369e-02 -3.88420001e-02\n",
            " -1.09261900e-01 -7.51080038e-03  1.19599532e-02  1.27546713e-02\n",
            "  1.89590380e-02  4.54232767e-02 -4.60909642e-02 -5.17158210e-03\n",
            " -1.17527824e-02 -8.67653918e-03 -2.08858345e-02  4.49374244e-02\n",
            "  1.55425835e-02  1.32864378e-02 -3.67459990e-02  1.40869170e-02\n",
            "  2.77772453e-03  2.77871266e-03  2.99190190e-02 -3.01352777e-02\n",
            " -4.63992432e-02 -5.60870133e-02 -7.94627424e-03  3.58323082e-02\n",
            " -2.37629116e-02  3.04554924e-02  4.38175211e-03 -1.49127860e-02\n",
            " -2.00192649e-02  4.84522851e-03 -1.40723865e-03 -3.53151895e-02\n",
            "  5.58805279e-03  7.45537179e-03  1.51485356e-03  4.03528996e-02\n",
            " -6.45001512e-03 -2.26511969e-03 -3.91197242e-02  1.05103217e-02\n",
            "  1.14451721e-02  2.85171885e-02  2.43227761e-02 -8.16608667e-02\n",
            " -4.06113975e-02  4.48722802e-02  5.76155318e-04  3.66368033e-02\n",
            " -5.07901385e-02  3.42644230e-02  2.49840338e-02  1.17401499e-02\n",
            "  1.71504412e-02  2.12810226e-02 -1.83073711e-02 -5.08700535e-02\n",
            " -1.79200340e-02  2.44996194e-02 -8.84234626e-03  1.70267001e-02\n",
            " -2.69820355e-03 -7.86307901e-02  5.88882677e-02  2.79416447e-03\n",
            "  1.18669895e-02 -3.29488665e-02  2.49917284e-02 -3.39025520e-02\n",
            " -7.46752918e-02  2.85451789e-03 -4.59523033e-03  1.36549887e-03\n",
            " -6.91541433e-02  3.54959094e-03 -1.40170185e-02  6.54007215e-03\n",
            " -5.49735799e-02  4.28330638e-02 -5.33593968e-02  3.18156206e-03\n",
            "  1.04328476e-01  3.42741422e-02  4.07343544e-02  1.89621579e-02\n",
            "  2.44271476e-02 -1.29662910e-02  6.00200109e-02  3.92833650e-02\n",
            "  7.58033171e-02 -1.51843773e-02 -7.98323378e-03  3.47588398e-02\n",
            " -1.86615046e-02 -6.96072355e-02 -7.13097379e-02  2.77238321e-02\n",
            " -3.20368260e-02  3.10048107e-02  1.26676972e-03 -6.69391209e-33\n",
            " -3.91474739e-02 -3.46212350e-02  2.06932495e-03  6.21101968e-02\n",
            " -4.16610613e-02 -9.90145002e-03 -1.67433228e-02  7.94485956e-03\n",
            " -1.07785943e-03  2.85013840e-02 -3.19682248e-02  1.79134973e-03\n",
            "  3.13649736e-02 -1.40697565e-02  1.93634443e-02  7.51154032e-03\n",
            "  3.52905132e-02 -1.16605721e-02 -2.80541973e-03 -1.19963875e-02\n",
            " -2.97140293e-02 -1.76579654e-02  4.52528074e-02 -1.38794514e-03\n",
            " -7.87162222e-03 -8.17419961e-03 -5.47760874e-02 -1.12037072e-02\n",
            " -6.26672581e-02 -2.15537120e-02  5.16276294e-03 -2.60675326e-02\n",
            " -1.97687466e-02 -2.41160523e-02 -3.39964777e-02  4.55972850e-02\n",
            " -5.38014621e-03 -5.15832677e-02  2.78135333e-02  3.86533514e-02\n",
            " -9.17188749e-02 -5.43299317e-02 -2.38128491e-02  8.47345591e-03\n",
            " -2.56151259e-02 -1.94259454e-02 -5.79072954e-03 -3.53576094e-02\n",
            "  3.68123986e-02 -4.75911982e-02 -3.93513069e-02  1.03630207e-03\n",
            " -3.56919914e-02  4.05901670e-02 -3.41653498e-03  2.35695764e-02\n",
            " -1.65533330e-02 -1.51561550e-03 -4.22695577e-02  1.85886826e-02\n",
            "  4.51937988e-02  5.00863902e-02 -3.62452343e-02 -3.38022858e-02\n",
            " -2.15226915e-02  7.74866343e-03  3.47941509e-03  8.42177251e-04\n",
            "  1.18841017e-02  6.97644800e-02  8.02968815e-03  1.04670852e-01\n",
            " -4.34279144e-02  1.09933443e-01  2.27688458e-02 -3.14176492e-02\n",
            " -1.14897387e-02 -3.55333183e-03  2.82175895e-02 -1.62152369e-02\n",
            "  6.32873699e-02  1.12805497e-02 -4.53989543e-02 -4.23893481e-02\n",
            " -4.77063432e-02 -4.93459441e-02 -3.72877251e-03  3.38707939e-02\n",
            " -3.09105702e-02  2.06780992e-02  3.08634974e-02  6.29141405e-02\n",
            "  1.70472804e-02 -1.72119867e-02 -3.77117582e-02  3.45211886e-02\n",
            " -4.09610979e-02  4.88864770e-03 -3.00612636e-02 -8.41374695e-03\n",
            " -4.09953296e-02 -3.98016870e-02 -5.39269410e-02  1.65642872e-02\n",
            "  5.96865080e-02  3.61520611e-02  4.98930141e-02  1.44997826e-02\n",
            " -1.09169655e-01 -1.43747842e-02 -1.36372177e-02  1.62525605e-02\n",
            " -1.17085304e-03 -3.09679732e-02 -2.90011913e-02 -6.66358601e-03\n",
            "  9.04124044e-03  4.31806929e-02 -2.07463559e-02 -5.69086969e-02\n",
            " -2.79607829e-02  4.16314118e-02 -6.23095296e-02  2.17974670e-02\n",
            "  2.10565981e-03  1.54056903e-02  3.57553475e-02  2.54537668e-02\n",
            "  3.60637158e-02 -7.28386566e-02 -5.19868033e-03 -2.23391922e-03\n",
            "  2.51205563e-07  4.48058033e-03  6.26779497e-02  2.36656684e-02\n",
            "  6.45827278e-02  1.77587718e-02  4.13445830e-02 -3.67187001e-02\n",
            "  5.56979440e-02 -4.12960052e-02  3.65489051e-02  7.52831325e-02\n",
            " -3.72790881e-02 -2.12006662e-02 -1.76451299e-02 -2.88425591e-02\n",
            "  2.56824456e-02 -4.92919907e-02 -8.79912227e-02 -2.83661876e-02\n",
            " -2.19022520e-02  3.70794870e-02  4.11574356e-02  7.84277841e-02\n",
            " -1.48524391e-02  6.14961144e-03 -4.01153900e-02 -2.02855971e-02\n",
            " -2.90953200e-02  6.01125276e-03  3.68368328e-02  7.31776096e-03\n",
            " -8.81802384e-03  4.70289867e-03  3.01263873e-02 -3.82545055e-03\n",
            " -6.81484537e-03  3.72340940e-02  8.78985450e-02 -2.90235388e-03\n",
            "  3.33462395e-02 -3.84543017e-02 -5.78215122e-02 -2.74080765e-02\n",
            "  1.45640476e-02  1.58608612e-02  1.84693802e-02  3.52275446e-02\n",
            " -5.63630722e-02  2.07086299e-02  3.22306864e-02 -2.99264453e-02\n",
            "  5.92911132e-02 -3.01271095e-03 -2.28289096e-03  2.80254874e-02\n",
            " -7.59407431e-02  4.06442676e-03  1.21565536e-02  1.28565114e-02\n",
            " -1.73884060e-03 -2.95145269e-02  3.77574749e-02  1.94634777e-02\n",
            "  4.80340719e-02  1.52996304e-02  5.04776724e-02 -8.81980825e-03\n",
            "  1.64886948e-34  4.77930866e-02 -6.48025097e-03 -3.31394537e-03\n",
            "  1.02902018e-02 -3.30803916e-02 -2.55397595e-02  3.78654450e-02\n",
            " -1.35549875e-02 -8.27929564e-03  2.65268572e-02 -2.01899558e-03]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the pretrained transformer model\n",
        "sentence_transformer_model = SentenceTransformer(\n",
        "    'sentence-transformers/{model}'.format(model=SENTENCE_TRANSFORMER_MODEL))\n",
        "\n",
        "# Sanity check\n",
        "example_encoding = sentence_transformer_model.encode(\n",
        "    \"This is an example sentence\",\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "print('example_encoding:\\n', example_encoding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "de0oQW1raJzY"
      },
      "outputs": [],
      "source": [
        "class TransformerFeaturizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, dim, sentence_transformer_model):\n",
        "        self.dim = dim\n",
        "        self.sentence_transformer_model = sentence_transformer_model\n",
        "        # you can add any other params to be passed to the constructor here\n",
        "\n",
        "    #estimator. Since we don't have to learn anything in the featurizer, this is a no-op\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    #transformation: return the encoding of the document as returned by the transformer model \n",
        "    def transform(self, X, y=None):\n",
        "        X_t = []\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Goal: TransformerFeaturizer's transform() method converts the raw text document\n",
        "        into a feature vector to be passed as input to the classifier.\n",
        "            \n",
        "        Given below is a dummy implementation that always maps it to a zero vector.\n",
        "        You have to implement this function so it uses computes a document embedding\n",
        "        of the input document using self.sentence_transformer_model. \n",
        "        This will be our feature representation of the document\n",
        "        \"\"\"\n",
        "        # for doc in X:\n",
        "        #     # TODO: replace this dummy implementation\n",
        "        #     X_t.append(np.zeros(self.dim))\n",
        "        # return X_t\n",
        "        return self.sentence_transformer_model.encode(X, normalize_embeddings=True, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n1nwMri8aeFi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for training data size = 500\n",
            "Accuracy on test set: 0.662\n",
            "Evaluating for training data size = 1000\n",
            "Accuracy on test set: 0.6774\n",
            "Evaluating for training data size = 10000\n",
            "Accuracy on test set: 0.7598\n",
            "Evaluating for training data size = 25000\n",
            "Accuracy on test set: 0.7716\n"
          ]
        }
      ],
      "source": [
        "models_v2 = {}\n",
        "for n in TRAIN_SIZE_EVALS:\n",
        "    print(\"Evaluating for training data size = {}\".format(n))\n",
        "    X_train_i = X_train[:n]\n",
        "    Y_train_i = Y_train[:n]\n",
        "\n",
        "    \"\"\"\n",
        "    [TO BE IMPLEMENTED]\n",
        "        \n",
        "    Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "    You have to replace it with a pipeline object which contains at least two steps:\n",
        "    (1) mapping the input document to a feature vector (using TransformerFeaturizer)\n",
        "    (2) a classifier that predicts the class label using the feature output of first step\n",
        "\n",
        "    You can add other steps to preproces, post-process your data as you see fit. \n",
        "    You can also try any sklearn model architecture you want, but a linear classifier\n",
        "    will do just fine to start with\n",
        "\n",
        "    e.g. \n",
        "    pipeline = Pipeline([\n",
        "        ('featurizer', <your TransformerFeaturizer class instance here>),\n",
        "        ('classifier', <your sklearn classifier class instance here>)\n",
        "    ])\n",
        "    \"\"\"\n",
        "    pipeline =  Pipeline([\n",
        "        ('vect', TransformerFeaturizer(dim=1024, sentence_transformer_model=sentence_transformer_model)),\n",
        "        ('clf', LogisticRegression(max_iter=1000, C=0.1))\n",
        "    ])\n",
        "\n",
        "    # train\n",
        "    pipeline.fit(X_train_i, Y_train_i)\n",
        "    # predict\n",
        "    Y_pred_i = pipeline.predict(X_test)\n",
        "    # record results\n",
        "    models_v2[n] = {\n",
        "        'pipeline': pipeline,\n",
        "        'test_predictions': Y_pred_i,\n",
        "        'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "        'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "        'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "    }\n",
        "    print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xw89vQkE5woo"
      },
      "source": [
        "## Step 5: [Modeling part 3] Large Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "jSijGuIc5yX4"
      },
      "outputs": [],
      "source": [
        "# Here's a couple of code snippets to help you familiarize with how to generate labels with LLMs using langchain,\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    # model_name='ada',\n",
        "    max_tokens=1000,\n",
        "    temperature=0.0,\n",
        "    request_timeout=120,\n",
        "    # It's better to do this an environment variable but putting it in plain text for clarity\n",
        "    openai_api_key = OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[88], line 14\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     ChatPromptTemplate,\n\u001b[1;32m      4\u001b[0m     SystemMessagePromptTemplate,\n\u001b[1;32m      5\u001b[0m     AIMessagePromptTemplate,\n\u001b[1;32m      6\u001b[0m     HumanMessagePromptTemplate,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     AIMessage,\n\u001b[1;32m     10\u001b[0m     HumanMessage,\n\u001b[1;32m     11\u001b[0m     SystemMessage\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m chat \u001b[39m=\u001b[39m ChatAnthropic()\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatAnthropic\n__root__\n  Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass  `anthropic_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatAnthropic\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatAnthropic()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "l5-u93SS50ip"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text='Positive' generation_info=None message=AIMessage(content='Positive', additional_kwargs={}, example=False)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "zero_shot_prompt_template = \"\"\"\n",
        "You are an expert at judging the sentiment of tweets. \n",
        "Your job is to categorize the sentiment of a given tweet into one of three categories: Positive, Negative, Neutral.\n",
        "\n",
        "Tweet: {tweet}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "prompt = zero_shot_prompt_template.format(\n",
        "    tweet=\"Yesss! I love machine learning\"\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "print(result.generations[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OovRQRuE52Tm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text='Positive' generation_info=None message=AIMessage(content='Positive', additional_kwargs={}, example=False)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "few_shot_prompt_template = \"\"\"\n",
        "You are an expert at judging the sentiment of tweets. \n",
        "Your job is to categorize the sentiment of a given tweet into one of three categories: Positive, Negative, Neutral.\n",
        "\n",
        "Some example tweets along with the correct sentiment are shown below.\n",
        "\n",
        "Tweet: Another big happy 18th birthday to my partner in crime. I love u very much!\n",
        "Sentiment: Positive\n",
        "\n",
        "Tweet: The more I use this application, the more I dislike it. It's slow and full of bugs.\n",
        "Sentiment: Negative\n",
        "\n",
        "Tweet: #Dreamforce Returns to San Francisco for 20th Anniversary. Learn more: http://bit.ly/3AgwO0H\n",
        "Sentiment: Neutral\n",
        "\n",
        "Now I want you to label the following example: \n",
        "Tweet: {tweet}\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "prompt = few_shot_prompt_template.format(\n",
        "    tweet=\"I like chocolate\"\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "print(result.generations[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nYou are an expert at judging the sentiment of tweets. \\nYour job is to categorize the sentiment of a given tweet into one of three categories: Positive, Negative, Neutral.\\n\\nSome example tweets along with the correct sentiment are shown below.\\n\\nTweet: Another big happy 18th birthday to my partner in crime. I love u very much!\\nSentiment: Positive\\n\\nTweet: The more I use this application, the more I dislike it. It's slow and full of bugs.\\nSentiment: Negative\\n\\nTweet: #Dreamforce Returns to San Francisco for 20th Anniversary. Learn more: http://bit.ly/3AgwO0H\\nSentiment: Neutral\\n\\nNow I want you to label the following example: \\nTweet: I like chocolate\\nSentiment:\\n\""
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "by3AxCi058qH"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "\n",
        "class LLMClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, llm_model, prompt_template):\n",
        "        self.llm_model = llm_model\n",
        "        self.prompt_template = prompt_template\n",
        "\n",
        "    #This will be called during the training step\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    #This will be called during inference.\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        [TO BE IMPLEMENTED]\n",
        "        \n",
        "        Goal: LLMClassifier's predict() method constructs the final prompt input\n",
        "        for the LLM for each x in X, using the prompt template.\n",
        "\n",
        "        You have to implement this function so it does the following:\n",
        "        1. Construct the final prompt for the LLM\n",
        "        2. Call `self.llm_model` to generate the completion (label) for the prompt\n",
        "        3. Do any post-processing/response parsing to fetch the label from the LLM response\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            # Construct the final prompt for the LLM\n",
        "            prompt = self.prompt_template.format(article = x)\n",
        "\n",
        "            # Call `self.llm_model` to generate the completion (label) for the prompt\n",
        "            label = self.llm_model.generate([[HumanMessage(content=prompt)]]).generations[0][0].text\n",
        "\n",
        "            # Do any post-processing/response parsing to fetch the label from the LLM response\n",
        "            predictions.append(label)\n",
        "\n",
        "        return predictions\n",
        "        # pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "news_category_zero_shot_template = \"\"\"\n",
        "You are an expert at categorizing the topics of different articles. \n",
        "Your job is to receive information about a news article, including title and summary and categorize the topic\n",
        "The possible topic categories are: 'Business', 'Sci/Tech', 'Software and Developement', 'Entertainment', 'Sports', 'Health', 'Toons' and 'Music Feeds'.\n",
        "\n",
        "I want you to label the following example: \n",
        "Article information: {article}\n",
        "Category:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=1000)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "CVemE-2R5-xN"
      },
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[95], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m pipeline\u001b[39m.\u001b[39mfit(X_train_i, Y_train_i)\n\u001b[1;32m     19\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m Y_pred_i \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m     21\u001b[0m \u001b[39m# record results\u001b[39;00m\n\u001b[1;32m     22\u001b[0m models_v3[\u001b[39m\"\u001b[39m\u001b[39mzero-shot\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtest_predictions\u001b[39m\u001b[39m'\u001b[39m: Y_pred_i,\n\u001b[1;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: accuracy_score(Y_true, Y_pred_i),\n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m: f1_score(Y_true, Y_pred_i, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     26\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m: \u001b[39msum\u001b[39m([x \u001b[39m!=\u001b[39m y \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(Y_true, Y_pred_i)])\n\u001b[1;32m     27\u001b[0m }\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sklearn/pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    480\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpredict(Xt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpredict_params)\n",
            "Cell \u001b[0;32mIn[46], line 32\u001b[0m, in \u001b[0;36mLLMClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_template\u001b[39m.\u001b[39mformat(article \u001b[39m=\u001b[39m x)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Call `self.llm_model` to generate the completion (label) for the prompt\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_model\u001b[39m.\u001b[39;49mgenerate([[HumanMessage(content\u001b[39m=\u001b[39;49mprompt)]])\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m     34\u001b[0m \u001b[39m# Do any post-processing/response parsing to fetch the label from the LLM response\u001b[39;00m\n\u001b[1;32m     35\u001b[0m predictions\u001b[39m.\u001b[39mappend(label)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:92\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     93\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[1;32m     94\u001b[0m generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:84\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     80\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     86\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     88\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:85\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(m, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     86\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     88\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/openai.py:323\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[1;32m    319\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    320\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion, \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: role}\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[0;32m--> 323\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/openai.py:282\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?"
          ]
        }
      ],
      "source": [
        "# Zero-shot classification pipeline with LLMs\n",
        "\n",
        "models_v3 = {}\n",
        "\n",
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "        \n",
        "Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "You have to replace it with a pipeline object which uses the `LLMClassifier` you have implemented \n",
        "above to perform zero-shot classification on the test set.\n",
        "\n",
        "You can add other steps to preproces, post-process your data as you see fit. \n",
        "\n",
        "\"\"\"\n",
        "pipeline = Pipeline([('LLMClassifier', LLMClassifier(llm, news_category_zero_shot_template))])\n",
        "\n",
        "# train\n",
        "pipeline.fit(X_train_i, Y_train_i)\n",
        "# predict\n",
        "Y_pred_i = pipeline.predict(X_test)\n",
        "# record results\n",
        "models_v3[\"zero-shot\"] = {\n",
        "    'test_predictions': Y_pred_i,\n",
        "    'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "    'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "    'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "}\n",
        "print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_i = X_train[:1000]\n",
        "Y_train_i = Y_train[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "news_category_few_shot_template = \"\"\"\n",
        "You are an expert at categorizing the topics of different articles. \n",
        "You receive info about a news article and have to categorize the topic.\n",
        "The possible topic categories are: 'Business', 'Sci/Tech', 'Software and Developement', 'Entertainment', 'Sports', 'Health', 'Toons' and 'Music Feeds'.\n",
        "\n",
        "Some examples below.\n",
        "\n",
        "Article information: EU to Rule Tuesday on Oracle's Bid for PeopleSoft. European Union regulators will decide Tuesday whether Oracle Corporation hostile $7.7 billion bid for rival business software concern PeopleSoft Inc. can proceed, the EU's antitrust chief said Friday.\n",
        "Category: Sci/Tech\n",
        "\n",
        "Article information: Capsule from Genesis Space Probe Crashes in Utah Desert. A capsule carrying solar material from the Genesis space probe has made a crash landing at a US Air Force training facility in the US state of Utah.\n",
        "Category: Entertainment\n",
        "\n",
        "\n",
        "I want you to label the following example: \n",
        "Article information: {article}\n",
        "Category:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PvCbruqf6ASV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-by3c5EoEWOFu8Vs6cwbhspzH on tokens per min. Limit: 90000 / min. Current: 89061 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-by3c5EoEWOFu8Vs6cwbhspzH on tokens per min. Limit: 90000 / min. Current: 88788 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-by3c5EoEWOFu8Vs6cwbhspzH on tokens per min. Limit: 90000 / min. Current: 89233 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
            "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-by3c5EoEWOFu8Vs6cwbhspzH on tokens per min. Limit: 90000 / min. Current: 89206 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[86], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m pipeline\u001b[39m.\u001b[39mfit(X_train_i, Y_train_i)\n\u001b[1;32m     18\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m Y_pred_i \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[1;32m     20\u001b[0m \u001b[39m# record results\u001b[39;00m\n\u001b[1;32m     21\u001b[0m models_v3[\u001b[39m\"\u001b[39m\u001b[39mfew-shot\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtest_predictions\u001b[39m\u001b[39m'\u001b[39m: Y_pred_i,\n\u001b[1;32m     23\u001b[0m     \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m: accuracy_score(Y_true, Y_pred_i),\n\u001b[1;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m: f1_score(Y_true, Y_pred_i, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m: \u001b[39msum\u001b[39m([x \u001b[39m!=\u001b[39m y \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(Y_true, Y_pred_i)])\n\u001b[1;32m     26\u001b[0m }\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/sklearn/pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    480\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpredict(Xt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpredict_params)\n",
            "Cell \u001b[0;32mIn[46], line 32\u001b[0m, in \u001b[0;36mLLMClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_template\u001b[39m.\u001b[39mformat(article \u001b[39m=\u001b[39m x)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Call `self.llm_model` to generate the completion (label) for the prompt\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_model\u001b[39m.\u001b[39;49mgenerate([[HumanMessage(content\u001b[39m=\u001b[39;49mprompt)]])\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[1;32m     34\u001b[0m \u001b[39m# Do any post-processing/response parsing to fetch the label from the LLM response\u001b[39;00m\n\u001b[1;32m     35\u001b[0m predictions\u001b[39m.\u001b[39mappend(label)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:92\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     93\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[1;32m     94\u001b[0m generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:84\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks)\u001b[0m\n\u001b[1;32m     80\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m     86\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     88\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/base.py:85\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[1;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(m, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     86\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m     87\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m     88\u001b[0m         \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages\n\u001b[1;32m     89\u001b[0m     ]\n\u001b[1;32m     90\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     91\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/openai.py:323\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager)\u001b[0m\n\u001b[1;32m    319\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    320\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion, \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: role}\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[0;32m--> 323\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/langchain/chat_models/openai.py:284\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Few-shot classification with LLMs\n",
        "\n",
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "        \n",
        "Goal: initialized below is a dummy sklearn Pipeline object with no steps.\n",
        "You have to replace it with a pipeline object which uses the `LLMClassifier` you have implemented \n",
        "above to perform few-shot classification on the test set.\n",
        "\n",
        "With few-shot classification, you can pass upto 5 demonstration examples as part of the prompt \n",
        "to the LLM. You can add other steps to preproces, post-process your data as you see fit. \n",
        "\n",
        "\"\"\"\n",
        "pipeline = Pipeline([('LLMClassifier', LLMClassifier(llm, news_category_few_shot_template))])\n",
        "\n",
        "# train\n",
        "pipeline.fit(X_train_i, Y_train_i)\n",
        "# predict\n",
        "Y_pred_i = pipeline.predict(X_test)\n",
        "# record results\n",
        "models_v3[\"few-shot\"] = {\n",
        "    'test_predictions': Y_pred_i,\n",
        "    'accuracy': accuracy_score(Y_true, Y_pred_i),\n",
        "    'f1': f1_score(Y_true, Y_pred_i, average='weighted'),\n",
        "    'errors': sum([x != y for (x, y) in zip(Y_true, Y_pred_i)])\n",
        "}\n",
        "print(\"Accuracy on test set: {}\".format(accuracy_score(Y_true, Y_pred_i)))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qdQDQ8Sla2u3"
      },
      "source": [
        "## Step 5: Report Results from previous two steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([(500, {'pipeline': Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', LogisticRegression(C=0.1, max_iter=1000))]), 'test_predictions': array(['Business', 'Entertainment', 'Entertainment', ..., 'Sports',\n",
              "       'Entertainment', 'Business'], dtype='<U25'), 'accuracy': 0.3434, 'f1': 0.23815090506518286, 'errors': 3283}), (1000, {'pipeline': Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', LogisticRegression(C=0.1, max_iter=1000))]), 'test_predictions': array(['Business', 'Entertainment', 'Sports', ..., 'Sports',\n",
              "       'Entertainment', 'Business'], dtype='<U25'), 'accuracy': 0.491, 'f1': 0.42194390708266594, 'errors': 2545}), (10000, {'pipeline': Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', LogisticRegression(C=0.1, max_iter=1000))]), 'test_predictions': array(['Business', 'Entertainment', 'Sports', ..., 'Sports',\n",
              "       'Entertainment', 'Business'], dtype='<U25'), 'accuracy': 0.6474, 'f1': 0.619593901371379, 'errors': 1763}), (25000, {'pipeline': Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', LogisticRegression(C=0.1, max_iter=1000))]), 'test_predictions': array(['Business', 'Entertainment', 'Sports', ..., 'Sports', 'Health',\n",
              "       'Business'], dtype='<U25'), 'accuracy': 0.7016, 'f1': 0.6909593714049721, 'errors': 1492})])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "IpKaurcHa0Vh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N-gram Models: \n",
            "Train size: 500  |  Accuracy: 0.3434  |  F1 score: 0.23815090506518286 |  Num errors: 3283\n",
            "Train size: 1000  |  Accuracy: 0.491  |  F1 score: 0.42194390708266594 |  Num errors: 2545\n",
            "Train size: 10000  |  Accuracy: 0.6474  |  F1 score: 0.619593901371379 |  Num errors: 1763\n",
            "Train size: 25000  |  Accuracy: 0.7016  |  F1 score: 0.6909593714049721 |  Num errors: 1492\n"
          ]
        }
      ],
      "source": [
        "# Report results\n",
        "\n",
        "print(\"N-gram Models: \")\n",
        "for train_size, result in models.items():\n",
        "    print(\"Train size: {0}  |  Accuracy: {1}  |  F1 score: {2} |  Num errors: {3}\".format(\n",
        "        train_size,\n",
        "        result['accuracy'],\n",
        "        result['f1'],\n",
        "        result['errors']\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Rztr78oD6EQp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained Transformer Models: \n",
            "Train size: 500  |  Accuracy: 0.662  |  F1 score: 0.6227476339069221 |  Num errors: 1690\n",
            "Train size: 1000  |  Accuracy: 0.6774  |  F1 score: 0.6389611475382116 |  Num errors: 1613\n",
            "Train size: 10000  |  Accuracy: 0.7598  |  F1 score: 0.7514273185090816 |  Num errors: 1201\n",
            "Train size: 25000  |  Accuracy: 0.7716  |  F1 score: 0.7632906461344938 |  Num errors: 1142\n"
          ]
        }
      ],
      "source": [
        "print(\"Pretrained Transformer Models: \")\n",
        "for train_size, result in models_v2.items():\n",
        "    print(\"Train size: {0}  |  Accuracy: {1}  |  F1 score: {2} |  Num errors: {3}\".format(\n",
        "        train_size,\n",
        "        result['accuracy'],\n",
        "        result['f1'],\n",
        "        result['errors']\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "b_sQOJW36Cqr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Large Language Models: \n"
          ]
        }
      ],
      "source": [
        "print(\"Large Language Models: \")\n",
        "for mode, result in models_v3.items():\n",
        "    print(\"Mode: {0}  |  Accuracy: {1}  |  F1 score: {2} |  Num errors: {3}\".format(\n",
        "        mode,\n",
        "        result['accuracy'],\n",
        "        result['f1'],\n",
        "        result['errors']\n",
        "    ))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4v882hLIa6gR"
      },
      "source": [
        "## Step 6: Data Augmentation [Optional]\n",
        "\n",
        "In this section, we want to explore how to augment data efficiently to your existing training data. This is a very empirical exercise with a less well-defined playbook which means this section of the project is going to be open ended. Let us first understand what we mean by efficiency here, and why it matters:\n",
        "\n",
        "### Performance Gain (G):\n",
        "We will measure performance gain from data augmentation as the improvement in model accuracy (reduction in num. errors) on the Test dataset as defined above. \n",
        "\n",
        "### Budget (K):\n",
        "We will measure \"budget\" as the number of additional rows augmentated to the original training dataset.  In this project, the universe of data from which you will select to add to your training set is Datasets['augment'] (and downstream X_augment, Y_augment).\n",
        "\n",
        "This data is already labeled of course, but in most real-world scenarios the additional data is typically unlabeled. In order to augment it to your training data, you have to get it annotated which incurs some cost in time & money. This is the motivation to consider budget as a metric.\n",
        "\n",
        "### Efficiency (E = G / K): \n",
        "Efficiency = Performance Gain (Reduction in num errors in test set) / Budget (Number of additional rows augmented to the training dataset)\n",
        "\n",
        "We want to get the maximum gain in performance, while incurring minimum annotation cost."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-a-kk3Pk6Jx-"
      },
      "source": [
        "\n",
        "\n",
        "We can always sample more data at random from the augmentation set, and this is probably the first thing to try. Can we be more intelligent with the data we choose to augment to the training dataset?\n",
        "\n",
        "**Idea 1**: Look at the test errors that the current model is making. How can this help us guide our \"data collection\" for augmentation? One possible idea is to select examples from the augmentation dataset that are similar to these errors and add them to the training data. Similarity can be approximated in many ways:\n",
        "1. [Jaccard distance between two texts](https://studymachinelearning.com/jaccard-similarity-text-similarity-metric-in-nlp/)\n",
        "2. L2 distance between mean word vectors (we already compute these features for the entire dataset using WordVectorFeaturizer)\n",
        "3. L2 distance between sentence transformer embedding (we already compute these features for the entire dataset using TransformerFeaturizer)\n",
        "  \n",
        "\n",
        "**Idea 2**: Compute model's predictions on the augmentation dataset, and include those examples to the training dataset that the model finds \"hard\" ? (a proxy for this would be to look at cases where the output score distribution across all labels has nearly identical scores for top two or three labels).\n",
        "\n",
        "**Idea 3**: Look at the test errors that the current model is making, and the distribution of these errors across labels. Select examples from the augmentation dataset that belong to these classes - adding more training data for labels that the curent model does not do well on, can improve performance (assuming label quality is good)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UVN6r0abKHI"
      },
      "outputs": [],
      "source": [
        "# Examine current test errors\n",
        "test_errors = []\n",
        "Y_pred_i = models[25000]['test_predictions']\n",
        "\n",
        "for idx, label in enumerate(Y_true):\n",
        "    if label != Y_pred_i[idx]:\n",
        "        test_errors.append((X_test[idx], label,  Y_pred_i[idx]))\n",
        "\n",
        "print(\"Number of errors in the test set: {}\".format(len(test_errors)))\n",
        "print(\"Example errors: [example, true label, predicted label]\")\n",
        "for i in range(10):\n",
        "    print(test_errors[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2LCwb4ibYsV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Your additional data augmentation explorations go here\n",
        "\n",
        "For instance, the pseudocode for Idea (1) might look like the following:\n",
        "\n",
        "Augmented = {}\n",
        "For e in test_errors:\n",
        "   1. X_nn, y_nn = k nearest neighbors to (e) from X_augment, y_augment\n",
        "   2. Add each (x, y) from (X_nn, y_nn) to Augmented\n",
        "\n",
        "Add the Augmented examples to the training set\n",
        "Train the new model and record performance improvements\n",
        "\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
