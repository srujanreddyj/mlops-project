{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In the project this week, we will focus on model performance monitoring for the news classification model that we deployed in week 3 (and developed in weeks 1 and 2)\n",
        "\n",
        "1. We will download and parse the training dataset, logs from prediction service that record inference traffic and annotations (ground truth labels for the inference traffic)\n",
        "2. We will set up basic monitoring for system health (traffic volume, latency, SLA violations)\n",
        "3. We will compute data and label drift for the inference traffic using a few different techniques (Chi-square statistic, KS-statistic, classifier-based drift detection)\n",
        "4. We will understand model performance as a function of time for the inference traffic, and any ties we can derive to detected drift\n",
        "5. [optional] We will experiment with outlier detection techniques and understand the impact of outliers on model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mu-AOZaimLTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1: Prereqs & Installation\n",
        "\n",
        "Download & Import all the necessary libraries we need throughout the project."
      ],
      "metadata": {
        "id": "ZfrFgbRqmW7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all the required dependencies for the project\n",
        "\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install scipy\n",
        "!pip install pandas\n",
        "!pip install umap-learn\n",
        "!pip install plotly"
      ],
      "metadata": {
        "id": "XBUyiD8KmUfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from datetime import date, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "from sklearn import metrics as sklearn_metrics\n"
      ],
      "metadata": {
        "id": "4a7mAa3_mZRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Constants\n",
        "\n",
        "LABEL_SET = [\n",
        "    'Business',\n",
        "    'Sci/Tech',\n",
        "    'Software and Developement',\n",
        "    'Entertainment',\n",
        "    'Sports',\n",
        "    'Health',\n",
        "    'Toons',\n",
        "    'Music Feeds'\n",
        "]\n",
        "\n",
        "DATA_URL = 'https://corise-mlops.s3.us-west-2.amazonaws.com/project4/agnews_logs.zip'\n",
        "\n",
        "LOG_DATE_START = date(2022, 7, 11)   # '2022-07-11'\n",
        "LOG_DATE_END = date(2022, 7, 24)   # '2022-07-24'"
      ],
      "metadata": {
        "id": "5G5PDLEcmbnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Download and Load Dataset\n",
        "\n",
        "In weeks 1 and 2, we worked with a modified version of the [AG News](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) dataset - a collection of more than 1 million news articles gathered from more than 2000 news sources by an academic news search engine. This news topic classification dataset & benchmark was first used in [Character-level Convolutional Networks for Text Classification (NIPS 2015)](https://arxiv.org/abs/1509.01626).\n",
        "\n",
        "In week 3, we started logging model inputs and predictions for the web application that we created for the trained news classifier.\n",
        "\n",
        "The dataset for this week's project is a combination of the two. We will work with the following files in the downloaded data:\n",
        "1. `training.json` -- this is the training data on which the classification model was trained. This will act as the reference dataset when we want to compute things like drift and outliers. Each row in this file is a training data point.\n",
        "2. `logs.json` -  this is a collection of logged outputs from our online service (the inference traffic). Each row in this file is a timestamped request, and contains the input request (text description, embedding, url etc) as well as model predictions. This will act as the target dataset when we want to compute things like drift and outliers. **The logs span a two week period from 2022/07/11 to 2022/07/24.**\n",
        "3. `annotations.json` - this is the set of ground truth labels for requests received by our online prediction service. Imagine we have a team of human annotators that label a fraction of our inference stream (with some delay). Eventually these ground truth labels are logged and will be helpful for us to monitor online model performance, and also is a good source of future training data for the model."
      ],
      "metadata": {
        "id": "2tscIRhUmgrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample row in the training dataset:\n",
        "```\n",
        "{\n",
        "   \"id\": 86273,\n",
        "   \"source\": \"Voice of America\",\n",
        "   \"title\": \"Capsule from Genesis Space Probe Crashes in Utah Desert\",\n",
        "   \"url\": \"http://www.sciencedaily.com/releases/2004/09/040908090621.htm\",\n",
        "   \"rank\": \"5\",\n",
        "   \"description\": \"A capsule carrying solar material from the Genesis space probe has made a crash landing at a US Air Force training facility in the US state of Utah.\",\n",
        "   \"embedding\": [...],\n",
        "   \"label\": \"Sci/Tech\"\n",
        "}\n",
        "```\n",
        "\n",
        "Sample row in the logs file:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"id\": 69265,\n",
        "    \"timestamp\": \"2022-07-11 00:00:00\",\n",
        "    \"host_id\": \"host_4\",\n",
        "    \"latency_ms\": 169.676,\n",
        "    \"request\": {\n",
        "        \"id\": 69265,\n",
        "        \"source\": null,\n",
        "        \"title\": \"Google May Face Another Lawsuit\",\n",
        "        \"url\": \"http://www.pcworld.com/news/article/0,aid,117686,00.asp\",\n",
        "        \"rank\": \"5\",\n",
        "        \"description\": \"A federal judge in Virginia has ruled that a trademark infringement suit filed by the Government Employees Insurance Co. (GEICO) against Internet search giants Google and Overture Services can proceed.\",\n",
        "        \"embedding\": [...]\n",
        "    },\n",
        "    \"pred_label\": \"Business\",\n",
        "    \"pred_score\": {\n",
        "        \"Business\": 0.39581484916169474,\n",
        "        \"Entertainment\": 0.19195937955028541,\n",
        "        \"Health\": 0.02007952252798203,\n",
        "        \"Music Feeds\": 0.0005983183076385058,\n",
        "        \"Sci/Tech\": 0.3849374113779283,\n",
        "        \"Software and Developement\": 0.001389120851963045,\n",
        "        \"Sports\": 0.004566606342231832,\n",
        "        \"Toons\": 0.0006547918802761208\n",
        "    }\n",
        "}\n",
        "```\n",
        "Sample row in the annotations file (rows in here should be joined to the correct request in `logs.json` using the \"id\" field):\n",
        "```\n",
        "{\n",
        "    \"id\": 69265,\n",
        "    \"label\": \"Sci/Tech\"\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "u68_CWU8miU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "DIRECTORY_NAME = \"data\"\n",
        "\n",
        "\n",
        "def download_dataset():\n",
        "    \"\"\"\n",
        "    Download the dataset. The zip contains three files: train.json, test.json and unlabeled.json\n",
        "    \"\"\"\n",
        "    http_response = urlopen(DATA_URL)\n",
        "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
        "    zipfile.extractall(path=DIRECTORY_NAME)\n",
        "\n",
        "\n",
        "# Expensive operation so we should just do this once\n",
        "download_dataset()\n"
      ],
      "metadata": {
        "id": "PzmOKJRCme4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "1. Load the training dataset, inference logs and annotations from the downloaded data\n",
        "2. Join the rows from `logs.json` and `annotations.json`.\n",
        "   Rows in these files should be matched using the \"id\" field - this is unique string assigned to each incoming request\n",
        "\"\"\"\n",
        "\n",
        "training = []\n",
        "\n",
        "# The logs span a two week period from 2022/07/11 to 2022/07/24.\n",
        "logs = []\n",
        "\n",
        "true_labels = []\n"
      ],
      "metadata": {
        "id": "HrZ4cJBzmrBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: System Health & Metrics\n",
        "\n",
        "As we saw in the lecture, software system failures and downtimes are remarkably common even in ML systems. In an [analysis](https://www.youtube.com/watch?v=hBMHohkRgAA) conducted by Daniel Papasian and Todd Underwood (both ML engineers at Google), they looked at large ML pipeline failures at Google and found out that in 60 out of the 96 cases, the cause for failure was not directly related to the ML model.\n",
        "\n",
        "For such cases, tracking system health can be a good first step to ensure properties such as whether the deployed model is available online, is its latency within acceptable SLAs, are the system resources (such as CPU and memory usage) within bounds etc.\n"
      ],
      "metadata": {
        "id": "KHeZKb84mt1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Each row in the logs.json file has an associated timestamp of the format YYYY-MM-DD HH:MM:SS.\n",
        "The logs span a two week period from 2022/07/11 to 2022/07/24.\n",
        "We will use the timestamp to group requests by date, and track a few system metrics of interest:\n",
        "\n",
        "1. Volume: Compute the daily volume of requests received by our service, grouped by host id.\n",
        "2. Latency: Compute the mean, median, P90, P95 latency of requests received by our service, grouped by host id\n",
        "3. Plot each of these as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "You can plot the metric for each host id in the same line chart (with different colors for each host).\n",
        "\n",
        "Do you notice anything strange with one of the hosts? :)\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "iVJzg1fCmsdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Let us say that we have a maximum latency SLA of 300 milliseconds i.e.\n",
        "if the prediction service took more than 300 ms to answer the request, the downstream experience is degraded for users.\n",
        "\n",
        "1. Compute the aggregate daily volume of latency SLA violations (i.e. number of requests that have a latency >= maximum allowed latency) grouped by each host id\n",
        "2. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "SqMJhbVqmv5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Model inputs and outputs\n",
        "\n",
        "In supervised machine learning settings, we assume that the training dataset D = {X, y} is composed of input/label tuples {Xi, yi} that are independently drawn from some underlying joint distribution ℙ(X, y) such that ℙ(X, y) = ℙ(y|X)ℙ(𝐗)\n",
        "\n",
        "ℙ(y|X) is the relationship we are trying to learn during the model training step, which can then be used to generate accurate predictions for unseen samples. We make two assumptions here:\n",
        "1. The unseen samples that the model will be used to make predictions on, comes from the same underlying distribution ℙ(X, y).\n",
        "2. This distribution ℙ(X, y) is stationary and does not change with time.\n",
        "\n",
        "In practice, as we saw in the lecture, this assumption does not hold in most cases. Can we track and quantify this change over time though?"
      ],
      "metadata": {
        "id": "X43pCxgSmy6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4[part 1]: Input drift (hypothesis testing)\n",
        "\n",
        "Hypothesis testing is a principled approach to evaluating drift. It’s a test to determine whether the difference between two populations (two sets of data) is statistically significant. If the difference is statistically significant, then the probability that the difference is a random fluctuation due to sampling variability is very low, and therefore, the difference is caused by the fact that these two populations come from two distinct distributions.\n",
        "\n",
        "1. Design a test statistic (or a distance metric) that is computed on samples collected form the two distributions - in our case, the reference and target distributions (i.e. data points from the inference and reference datasets)\n",
        "2. The test statistic is expected to be small if the null hypothesis is true (i.e. Z and Zref are drawn from the same distribution), and large if the alternative hypothesis (i.e. Z and Zref are drawn from different distributions) is true.\n",
        "3. From the test statistic, we compute a p-value: When p-value ≤ threshold, results from the test are said to be statistically significant, and the null hypothesis P(z) = Pref(z) is rejected.\n"
      ],
      "metadata": {
        "id": "Ua2Iaczlm0UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "News source (e.g. New York Times, Reuters, BBC etc) is an important piece of metadata about the incoming requests.\n",
        "Different news sources cover different topics, and any shift in the distribution/prevalence of news sources\n",
        "can be an important early signal to suggest that online traffic patterns might be changing.\n",
        "\n",
        "1. Both, the training dataset and infernce logs contains the news source metadata.\n",
        "2. Using the training dataset as the reference, quantify the drift in news sources for incoming requests.\n",
        "   You will compute drift for each day, using the inference logs from that day (i.e. \"target\") and comparing it to the training dataset (i.e. \"reference\")\n",
        "3. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\n",
        "Which metric should you use to measure drift though? Chi-squared statistic is commonly used to measure drift for categorical features as we discussed.\n",
        "\n",
        "Refer to:\n",
        "1) https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html\n",
        "2) https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/chisquaredrift.html (in turn uses scipy under the hood)\n",
        "\n",
        "These implementations will return the distance, and the p-value of the null hypothesis as part of the response\n",
        "You want to plot the p-value in the line chart.\n",
        "\n",
        "MAKE SURE YOU DEAL WITH MISSING/NULL/NONE VALUES IN THE TRAINING AND INFERENCE DATA IN A CONSISTENT WAY\n",
        "\"\"\"\n",
        "\n",
        "news_sources_ref = [row['source'] for row in training]\n",
        "delta = LOG_DATE_END - LOG_DATE_START\n",
        "\n",
        "news_sources_drift = []\n",
        "\n",
        "for d in range(delta.days + 1):\n",
        "   curr_date = LOG_DATE_START + timedelta(days=d)\n",
        "   print(curr_date)\n",
        "\n",
        "   # Step 1: collect all requests from logs.json with timestamp on the curr_date\n",
        "   # this is currently a dummy empty list\n",
        "   news_sources_target = []\n",
        "\n",
        "   # Step 2: compute the drift between `news_sources_target` and `news_sources_ref` using Chi squared statistic\n",
        "   # you can use either the scipy.stats.chisquare or the implementation in alibi-detect as shared in the references above\n",
        "\n",
        "   # Step 3: store the pvalue in `news_sources_drift`\n",
        "\n",
        "\n",
        "# Step 4:  Plot the `news_sources_drift` as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n"
      ],
      "metadata": {
        "id": "CW0Vf0a3mxIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "It seems like there is some divergence in the distributions of the news sources feature between the reference and target set on some days.\n",
        "Let's visualize this:\n",
        "\n",
        "Let's take a look at the last day of inference traffic (you might observe p-value << 0.05 in the cell above for this day).\n",
        "Plot a bar chart with the news sources on the X-axis, and normalized frequences in the reference and target datasets on the Y-axis\n",
        "\n",
        "MAKE SURE TO NORMALIZE THE FREQUENCIES SINCE THE RAW NUMBER OF OBSERVATIONS IN BOTH LISTS WILL NOT BE THE SAME\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def plot_chart(news_sources: List, freq_reference: List, freq_target: List):\n",
        "    X_axis = np.arange(len(news_sources))\n",
        "    plt.bar(X_axis - 0.2, freq_reference, 0.4, label='Reference')\n",
        "    plt.bar(X_axis + 0.2, freq_target, 0.4, label='Target')\n",
        "\n",
        "    plt.xticks(X_axis, news_sources)\n",
        "    plt.xlabel(\"News Sources\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# You can plot both the bar charts together by calling the `plot_chart` function\n",
        "plot_chart(\n",
        "    news_sources=['dummy_source_1', 'dummy_source_2', 'dummy_source_3'],\n",
        "    freq_reference=[0.3, 0.5, 0.2],\n",
        "    freq_target=[0.3, 0.4, 0.3],\n",
        ")\n"
      ],
      "metadata": {
        "id": "78PNIC0dm4t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4[part 2]: Input drift (Classifier based)\n",
        "\n",
        "The previous approach of treating drift detection as a two-sample hypothesis test is based on designing the correct test statistic. Oftentimes it can be hard to choose the correct statistic, and most test statistics are prone to false positives especially for multivate high dimensional data.\n",
        "\n",
        "What if we instead treat this as a classification problem? Can we train a classifier to predict which of the two distributions (reference or the target distribution) a given data point came from? The basic intuition is that if a classifier can learn to discriminate between the two distributions significantly better than random, then drift must have occurred.\n",
        "\n",
        "The classifier-based drift detector tries to correctly distinguish instances from the reference dataset vs. the target dataset by training a classifier. It is possible to consume outputs of the classifier in a few different ways.\n",
        "\n",
        "1. We can binarize the classifier prediction score based on a decision threshold, and apply a binomial test on the binarized predictions of the reference vs. the target data.\n",
        "2. We can use the classifier prediction score directly and compare the different in score distributions for the reference and target datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "pkPSzVwxnCGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Embeddings represent an encoding of the semantic content of the inputs to the model, and also the feature space on\n",
        "top of which we train the classifier. Any shift in the space of embeddings can be an important signal to suggest\n",
        "that online traffic patterns might be changing which can impact downstream model performance\n",
        "\n",
        "1. Both, the training dataset and infernce logs contain embedding representations of the news article description.\n",
        "2. Using the training dataset as the reference, quantify the drift in embeddings for incoming requests.\n",
        "   You will compute drift for each day, using the inference logs from that day (i.e. \"target\") and comparing it to the training dataset (i.e. \"reference\")\n",
        "3. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\n",
        "We will implement classifer-based drift detection for embeddings. You are free to choose the architecture of the classifier,\n",
        "but feel free to go with something simple (e.g. logistic regression, or a neural network with one hidden layer).\n",
        "\n",
        "Remember, this is a binary classifier that tries to predict whether a given data point belongs to the reference or the target distribution.\n",
        "\n",
        "Also, remember that a new instance of the classifier will be trained for each (referce, target) pair i.e. for each day\n",
        "\n",
        "Refer to:\n",
        "1) https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/classifierdrift.html\n",
        "2) https://openreview.net/forum?id=SJkXfE5xx\n",
        "\n",
        "These implementations will return the distance, and the p-value of the null hypothesis as part of the response\n",
        "You want to plot the p-value in the line chart.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "embeddings_ref = [row['embedding'] for row in training]\n",
        "\n",
        "delta = LOG_DATE_END - LOG_DATE_START\n",
        "embeddings_drift = []\n",
        "\n",
        "for d in range(delta.days + 1):\n",
        "   curr_date = LOG_DATE_START + timedelta(days=d)\n",
        "   print(curr_date)\n",
        "\n",
        "   # Step 1: collect all requests from logs.json with timestamp on the curr_date\n",
        "   # this is currently a dummy empty list\n",
        "   embeddings_target = []\n",
        "\n",
        "   # Step 2: compute the drift between `embeddings_target` and `embeddings_ref`\n",
        "   # (i) Initialize a new instance of the drift model (e.g. LogisticRegression())\n",
        "   # (ii) Initialize the drift detector (check out https://docs.seldon.io/projects/alibi-detect/en/stable/cd/methods/classifierdrift.html)\n",
        "   # (iii) use the initialized drift detector to compute the p-value\n",
        "\n",
        "   # Step 3: store the pvalue in `embeddings_drift`\n",
        "\n",
        "\n",
        "# Step 4:  Plot the `embeddings_drift` as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_WyzQeam_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "It seems like there is some divergence in the distributions of embeddings between the reference and target set on some days.\n",
        "Let's visualize with UMAP (https://umap-learn.readthedocs.io/en/latest/). Uniform Manifold Approximation and Projection (UMAP)\n",
        "is a dimension reduction technique that can be used for visualisation similarly to t-SNE.\n",
        "\n",
        "1. We will train a UMAP model to project our original embedding space into 2 dimensions, using our reference (training) dataset.\n",
        "2. We will then use this trained model to map the reference and the target dataset into two dimensions, and visualize it as a scatter plot.\n",
        "\n",
        "Do this exercise separately for two target distributions:\n",
        "1. inference traffic from 2022/07/11 (no drift)\n",
        "2. inference traffic from 2022/07/24 (maximum drift)\n",
        "\"\"\"\n",
        "\n",
        "def train_umap_model(emb):\n",
        "    umap_model = umap.UMAP(n_components=2)\n",
        "    umap_model.fit(emb)\n",
        "    return umap_model\n",
        "\n",
        "\n",
        "umap_model = train_umap_model(embeddings_ref)\n",
        "umap_ref = umap_model.transform(embeddings_ref)\n",
        "\n",
        "# Similarly, use the umap_model.transform(...) function to compute 2d representations of inference traffic\n",
        "umap_target_0711 = umap_model.transform(...)\n",
        "umap_target_0724 = umap_model.transform(...)\n"
      ],
      "metadata": {
        "id": "BhAxxNqjnHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "final_df = []\n",
        "for idx, ut in enumerate(umap_ref[:5000]):\n",
        "    final_df.append([ut[0], ut[1], 'reference'])\n",
        "\n",
        "for idx, ut in enumerate(umap_target_0711):\n",
        "    final_df.append([ut[0], ut[1], 'target_2022-07-11'])\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    final_df, columns=[\"x\", \"y\", \"label\"]\n",
        ")\n",
        "\n",
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\")\n",
        "fig.update_traces(marker={'size': 3})\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Nna9k3bInIzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "final_df = []\n",
        "for idx, ut in enumerate(umap_ref[:5000]):\n",
        "    final_df.append([ut[0], ut[1], 'reference'])\n",
        "\n",
        "for idx, ut in enumerate(umap_target_0724):\n",
        "    final_df.append([ut[0], ut[1], 'target_2022-07-24'])\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    final_df, columns=[\"x\", \"y\", \"label\"]\n",
        ")\n",
        "\n",
        "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\")\n",
        "fig.update_traces(marker={'size': 3})\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "CwsgicgpnJZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Model performance\n",
        "\n",
        "Ground truth observations (true labels) are a critical part of all supervised/self-supervised machine learning. Ground truth is important not just for model training, but also for monitoring and measuring performance of models after deployment.\n",
        "\n",
        "`annotations.json` is the set of ground truth labels for requests received by our online prediction service. Imagine we have a team of human annotators that label a fraction of our inference stream (with some delay). Eventually these ground truth labels are logged and will be helpful for us to monitor online model performance.\n"
      ],
      "metadata": {
        "id": "GqWgcpmInT5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Weekly classification report\n",
        "\n",
        "Compute the a classification report for each week of the logged inference data\n",
        "   Week 1 = [2022-07-11 and 2022-07-17]\n",
        "   Week 2 = [2022-07-18 and 2022-07-24]\n",
        "\n",
        "Do you notice a trend in model performance?\n",
        "How does this correlate to observed drift in news source, embeddings etc ?\n",
        "\"\"\"\n",
        "\n",
        "from sklearn import metrics as sklearn_metrics\n",
        "\n",
        "print(\"Dates: 2022/07/11 to 2022/07/17\")\n",
        "print(sklearn_metrics.classification_report([], []))\n",
        "\n",
        "print(\"Dates: 2022/07/18 to 2022/07/24\")\n",
        "print(sklearn_metrics.classification_report([], []))\n"
      ],
      "metadata": {
        "id": "bZfLtCZAnTb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Daily aggregate classification accuracy\n",
        "\n",
        "1. Compute the daily aggregate classification accuracy of the model\n",
        "2. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\n",
        "Do you notice a trend in model performance?\n",
        "How does this correlate to observed drift in news source, embeddings etc ?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OwlhT6JBnX8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Daily classification accuracy, grouped by label\n",
        "\n",
        "1. Compute the daily classification accuracy of the model, grouped by the true label category\n",
        "2. Plot each of these as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "   You can plot them on the same chart, with different colors for each label\n",
        "\n",
        "Do you notice a trend in model performance?\n",
        "Are there labels that degrade more quickly or less quickly?\n",
        "Are there labels that consistently perform worse?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Ppzrg1bDnZg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Step 6: Outliers & its impact on model performance\n",
        "\n",
        "A common cause for ML model failures is outliers - data samples that are very different or “unlike” the rest of the samples, and hence unlikely to be seen by the model during training. Doing well on such outliers is especially critical for high stakes applications like self-driving cars and medical diagnostics\n",
        "\n",
        "Unfortunately, it's not very easy to detect outliers because it's hard to constitute the criteria for an outlier. Typically, outlier detection algorithms fit (ex. via reconstruction) to the training set to understand what normal data looks like and then we can use a threshold to predict outliers.\n",
        "\n",
        "Libraries such as [PyOD](https://pyod.readthedocs.io/en/latest/) and [Alibi Detect](https://docs.seldon.io/projects/alibi-detect/en/latest/) are popular open source libraries for outlier detection.\n"
      ],
      "metadata": {
        "id": "gtCqX5TknbmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Consider using libraries shared above to identify the most outlier examples\n",
        "from the inference traffic, with respect to the training dataset.\n",
        "\n",
        "Some approaches/architectures you might consider:\n",
        "1. Local Outlier Factor or Isolation Forest (https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
        "2. Variational Auto encoder (https://github.com/yzhao062/pyod)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pHlFLFdincO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[TO BE IMPLEMENTED]\n",
        "\n",
        "Measure the model performance (model accuracy) on the subset of data that is identified as outliers.\n",
        "\n",
        "1. Compute the daily aggregate classification accuracy of the model on the subset of data that is identified as outliers.\n",
        "2. Plot this metric as a line chart, with date on the X-axis and the computed metric on the Y-axis.\n",
        "\n",
        "How does the model's performance on outliers compare to that on the inliers?\n",
        "Do you see model performance is lower for outliers?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "dIanQ3jAndWB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}